{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75057353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839da38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\surai/.cache\\torch\\hub\\v0.10.0.zip\n",
      "d:\\msc_dwm_lab_2024\\msc_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\msc_dwm_lab_2024\\msc_env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\surai/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:01<00:00, 33.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac1db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open(\"dog1.png\").convert(\"RGB\")\n",
    "\n",
    "# Display the image\n",
    "input_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f59af977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 841)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Print size (width, height)\n",
    "#print(\"Image size (width, height):\", input_image.size)\n",
    "input_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae10b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB\n"
     ]
    }
   ],
   "source": [
    "print(input_image.mode)  # 'RGB' means 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f38843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(841, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array(input_image)\n",
    "print(arr.shape)  # (height, width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea7a13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape (C, H, W): torch.Size([3, 841, 600])\n"
     ]
    }
   ],
   "source": [
    "# Convert image to tensor to get shape (channels, height, width)\n",
    "to_tensor = transforms.ToTensor()\n",
    "tensor_image = to_tensor(arr)\n",
    "print(\"Tensor shape (C, H, W):\", tensor_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8293fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) \n",
    "# create a mini-batch as expected by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13f35178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model =torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "model1 =torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model2 =torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd36e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.3039e-02,  2.0289e-01,  1.0851e-02, -5.3328e-02, -1.8357e-02,\n",
      "         1.2969e-01,  4.6258e-02,  1.4924e-02, -1.4557e-01,  2.8459e-02,\n",
      "        -2.9934e-02, -4.6462e-02,  3.6979e-04,  2.1700e-03, -1.8853e-02,\n",
      "        -1.6025e-01,  1.0510e-01, -7.8698e-02, -1.8121e-01,  8.1083e-03,\n",
      "         1.2476e-03, -1.0622e-01, -2.4858e-02, -1.1448e-01, -6.9394e-02,\n",
      "        -3.6537e-01, -1.0436e-01, -2.0785e-01, -5.5693e-02,  2.9380e-01,\n",
      "        -1.2627e-01, -4.1091e-02, -1.0654e-01,  1.4842e-02,  7.7031e-02,\n",
      "         6.5768e-02,  1.9930e-01, -2.9817e-01, -2.5198e-02, -2.2290e-02,\n",
      "        -7.8011e-02, -2.3833e-01, -2.2395e-01, -2.5195e-02, -2.0729e-02,\n",
      "        -1.3117e-01, -1.6026e-01, -4.4122e-03, -1.3790e-01, -3.1472e-02,\n",
      "        -1.6148e-02,  7.4491e-02,  1.3055e-04, -7.7674e-02, -2.5494e-01,\n",
      "        -1.5375e-04, -1.4990e-01, -1.4813e-01, -5.4143e-02, -1.0489e-02,\n",
      "        -1.5041e-01,  1.2727e-03, -3.8506e-02, -6.3158e-02,  1.1976e-01,\n",
      "        -7.2188e-02, -2.6792e-02, -1.7065e-01, -1.3273e-01,  4.6817e-02,\n",
      "        -2.2258e-01, -1.7231e-01, -3.2409e-02, -9.2675e-02, -1.0947e-01,\n",
      "         7.7515e-02,  2.1789e-01, -2.3420e-01, -8.5188e-02, -8.7131e-02,\n",
      "        -8.7342e-02, -2.5861e-01, -1.2385e-01, -1.0684e-01,  7.9353e-02,\n",
      "        -1.3217e-01, -1.6123e-01,  1.5358e-01,  1.4549e-02, -5.7663e-02,\n",
      "        -1.0125e-01, -1.6542e-01,  4.8735e-03, -1.4107e-01,  3.5834e-01,\n",
      "        -7.0741e-02,  5.6585e-02, -7.8065e-02, -1.8221e-01, -1.2459e-01,\n",
      "         7.1568e-02, -6.3933e-02, -2.1695e-01,  2.5208e-01, -2.8307e-01,\n",
      "        -1.0180e-01, -9.8206e-02,  4.5273e-02, -3.5536e-02, -2.2220e-01,\n",
      "        -9.9000e-02,  3.9201e-01, -2.5903e-02, -5.6978e-02, -1.5206e-01,\n",
      "         4.6589e-02, -5.4989e-02,  6.1700e-02, -5.7711e-02, -1.5543e-01,\n",
      "        -2.5201e-01,  4.4479e-02, -6.4134e-02, -1.7043e-01, -8.7153e-02,\n",
      "        -1.0550e-01, -3.7125e-02, -1.7881e-01, -2.4221e-01, -7.6457e-02,\n",
      "        -2.9807e-02, -1.5108e-01, -5.5683e-02, -9.3581e-02, -4.9862e-03,\n",
      "        -2.0187e-01, -8.8665e-02, -1.5837e-03, -1.8879e-01, -2.0884e-01,\n",
      "        -1.4494e-01, -4.5736e-02, -2.1466e-01, -1.6990e-01, -1.3907e-01,\n",
      "         1.2782e-02, -1.4613e-01, -9.9016e-02, -1.5620e-01,  1.4573e-01,\n",
      "         1.2420e-01,  2.9756e-01,  3.1925e-02,  3.0723e-01,  6.2647e-02,\n",
      "         1.9088e-02,  2.7728e-01,  3.8029e-02,  1.9125e-01,  1.7750e-01,\n",
      "         4.1749e-01, -1.1963e-02,  3.6577e-01,  5.9986e-02,  1.8162e-01,\n",
      "         1.4158e-01,  3.1381e-01, -9.2658e-02,  1.8549e-01,  1.5367e-02,\n",
      "         2.7112e-02,  2.5829e-01, -1.8573e-02, -2.1225e-01,  1.0666e-01,\n",
      "         2.8240e-02,  6.1727e-01, -2.5526e-02,  1.2978e-01, -8.0280e-02,\n",
      "        -5.5462e-03,  2.1296e-01,  1.0713e-01, -3.3356e-02,  4.8297e-02,\n",
      "        -6.4008e-02,  3.7974e-02, -7.4339e-02,  5.9298e-02,  6.0787e-02,\n",
      "         2.8700e-02,  1.0030e-01,  9.7474e-02, -1.1395e-02,  5.0958e-02,\n",
      "         7.5643e-02,  2.7695e-02, -3.2981e-02, -3.0769e-02,  7.4000e-02,\n",
      "         2.0214e-01, -6.0799e-02, -1.0958e-01, -5.3035e-02,  1.4107e-01,\n",
      "         3.3724e-01,  1.7827e-01,  6.8556e-01,  5.3770e-01,  3.1099e-02,\n",
      "        -6.5900e-02, -1.5684e-01,  2.6490e-01, -6.2275e-02,  4.2745e-01,\n",
      "         9.0710e-02,  8.9828e-02, -1.5993e-01,  2.8499e-02,  4.4632e-01,\n",
      "         1.2374e-01, -1.5793e-01,  4.4439e-01,  1.4085e-01, -5.6636e-03,\n",
      "        -9.3847e-02, -4.4860e-02, -4.1031e-02, -1.5080e-03,  8.4860e-02,\n",
      "         1.7943e-01,  1.7229e-01, -9.7022e-02, -1.3375e-02, -1.4082e-01,\n",
      "         3.8105e-02,  1.9165e-01,  8.5202e-02, -1.6250e-02,  8.0361e-02,\n",
      "         1.2189e-01,  9.0314e-02,  1.2527e-02, -2.6008e-02,  6.4931e-02,\n",
      "        -1.7327e-01,  1.5924e-01,  1.6261e-01,  1.6502e-01, -6.3595e-02,\n",
      "         8.4656e-02,  1.8945e-01, -1.0656e-01,  2.7423e-01, -8.7640e-02,\n",
      "         7.8344e-02,  2.2617e-01,  4.0987e-01,  5.9301e-02,  2.0874e-01,\n",
      "         2.0631e-01, -3.4040e-02, -2.4607e-01,  1.6616e-01, -1.7960e-02,\n",
      "         2.2915e-01,  5.3320e-01,  4.3534e-01, -7.1298e-02, -1.4759e-01,\n",
      "        -2.9380e-02, -2.5210e-02,  4.3846e-02,  2.0092e-01, -1.1136e-01,\n",
      "        -1.4049e-01, -3.0372e-02, -5.2022e-02, -2.3346e-01, -9.4667e-02,\n",
      "        -8.2302e-02, -2.6146e-02,  8.4575e-02,  1.2061e-01,  6.0622e-02,\n",
      "         9.1681e-02, -1.6120e-01, -1.4752e-01, -1.8804e-01, -1.1841e-01,\n",
      "        -1.1016e-01, -1.8167e-01, -1.1908e-01, -1.3020e-01, -1.3364e-01,\n",
      "        -1.8110e-01,  1.6008e-01,  6.4574e-02, -6.3810e-02, -3.1076e-02,\n",
      "        -1.7092e-01, -9.1174e-02, -2.2790e-01, -1.4042e-01, -7.2580e-02,\n",
      "        -2.2519e-01, -7.1507e-02, -1.4976e-01, -1.0754e-01,  3.4738e-02,\n",
      "         1.4985e-02, -1.0909e-01, -2.2184e-01, -1.4277e-02, -5.1229e-02,\n",
      "        -1.5747e-01, -2.0395e-01, -1.7344e-01, -1.1636e-01, -1.3744e-01,\n",
      "        -3.3769e-02, -1.6718e-01, -7.1712e-02, -1.4242e-01, -7.7253e-02,\n",
      "        -1.5537e-01, -8.7090e-02, -6.6207e-02, -7.7983e-03, -3.4772e-02,\n",
      "        -1.7239e-01, -8.3531e-02,  4.6436e-01,  1.4605e-01,  2.6867e-01,\n",
      "        -1.9709e-01, -1.3368e-01, -7.8589e-02, -4.8830e-02, -1.1861e-01,\n",
      "         7.3231e-02,  7.0668e-02, -1.8570e-01, -8.5944e-02,  1.3082e-02,\n",
      "         4.0113e-03, -2.1638e-02,  1.6905e-01, -7.1808e-02, -1.4650e-01,\n",
      "        -3.1245e-01, -1.9138e-01, -1.4176e-01, -7.6459e-02, -1.2019e-01,\n",
      "         8.9519e-02,  1.1474e-01,  6.6378e-03, -5.9247e-02,  2.2886e-01,\n",
      "        -3.2046e-02,  1.1772e-01,  1.5975e-01,  7.9065e-02, -4.7260e-02,\n",
      "        -3.2499e-02,  1.2208e-01,  1.4097e-01,  6.2101e-02,  2.6059e-01,\n",
      "        -3.5393e-02, -1.3443e-01,  7.0600e-02, -3.4671e-01, -1.0457e-02,\n",
      "         5.1849e-02, -1.8395e-01,  4.0658e-02, -5.9688e-02, -1.0015e-01,\n",
      "         1.5397e-01,  2.3291e-03,  7.6176e-03, -6.3195e-02, -2.9720e-01,\n",
      "        -8.3262e-02,  1.8452e-02, -5.6726e-02,  8.4021e-02,  7.8602e-02,\n",
      "        -7.0760e-02,  3.7533e-02, -8.5078e-02,  1.8230e-03,  2.3935e-01,\n",
      "         2.1346e-01, -1.3052e-02,  1.1859e-01, -7.0323e-02,  2.3783e-02,\n",
      "        -1.2877e-01,  6.1763e-02, -2.5288e-02,  4.0157e-02, -9.3047e-02,\n",
      "        -5.1245e-02,  2.9680e-02,  9.8394e-02,  2.5300e-02, -1.1078e-02,\n",
      "         6.4903e-02, -1.1156e-01,  2.0606e-01,  6.0531e-02, -1.4661e-01,\n",
      "        -1.2016e-01,  6.9422e-02,  4.7718e-02, -1.9717e-01,  4.3295e-01,\n",
      "         2.0950e-01,  2.9521e-02, -1.4108e-03,  6.1194e-02, -3.8906e-02,\n",
      "        -5.9375e-02, -1.7799e-01,  6.8922e-02, -6.3409e-02, -8.6114e-02,\n",
      "        -5.5073e-02,  8.3465e-02,  1.2837e-01,  3.5385e-02,  2.0810e-01,\n",
      "         1.2523e-02,  1.4678e-01, -2.2693e-01, -5.1448e-02,  1.0794e-01,\n",
      "        -1.8607e-01,  1.8839e-01, -1.8892e-02,  3.0891e-02, -1.1710e-01,\n",
      "         1.0378e-01, -8.1862e-02, -4.7218e-02, -5.7016e-03,  1.3464e-02,\n",
      "        -5.4089e-02, -1.1822e-01,  2.2401e-01, -1.7527e-01,  4.6118e-02,\n",
      "         9.8411e-02, -1.1646e-01,  3.0815e-01, -1.8534e-01,  1.2298e-01,\n",
      "         1.7737e-02,  1.8394e-01,  1.9118e-01, -8.0472e-02, -1.7028e-01,\n",
      "         6.3879e-02,  5.7566e-02, -3.9587e-02,  5.6908e-02,  2.1264e-01,\n",
      "         1.6803e-01,  1.1135e-01, -6.2555e-03, -1.2161e-01,  8.6281e-02,\n",
      "         5.3510e-01, -3.3105e-02,  1.6425e-01,  6.6969e-02,  2.3493e-01,\n",
      "         1.5424e-01,  2.7018e-02, -6.4364e-02, -1.4973e-01, -1.7507e-01,\n",
      "        -6.9083e-02, -2.0658e-02,  1.0138e-01, -3.0384e-02, -7.0459e-02,\n",
      "         2.4307e-03, -2.6648e-02, -1.1548e-01, -1.6561e-01,  9.0085e-02,\n",
      "        -2.4091e-01,  1.8725e-01, -1.1969e-01, -1.6322e-01,  4.9854e-02,\n",
      "        -1.1357e-01,  9.4883e-02,  1.2588e-01,  1.6160e-01,  1.3498e-01,\n",
      "        -2.1546e-01, -1.0565e-02,  5.9135e-02,  9.5034e-02, -2.7391e-01,\n",
      "        -1.0820e-01,  5.1537e-02, -3.0159e-01, -1.1725e-03,  1.4849e-01,\n",
      "         1.2612e-01,  1.4116e-01,  5.2456e-02, -7.0861e-02,  1.6091e-01,\n",
      "         1.4106e-01, -1.6290e-01, -2.0241e-01, -1.3572e-02,  2.0370e-01,\n",
      "         2.3625e-01, -2.5634e-02, -8.9609e-02, -6.6539e-02, -7.9763e-02,\n",
      "         2.8408e-01, -1.3683e-01, -4.3213e-02, -1.7114e-01, -4.1615e-04,\n",
      "         1.5143e-01, -1.6099e-01, -2.2407e-02, -2.3937e-02, -3.6611e-02,\n",
      "        -1.0888e-01, -7.2791e-02,  1.4061e-01,  2.0836e-01, -1.1897e-01,\n",
      "         2.1921e-01,  4.1955e-02, -5.7337e-02, -2.8270e-01,  1.0264e-01,\n",
      "        -1.0902e-01, -4.1299e-02,  4.3273e-01, -1.5529e-01, -4.4945e-04,\n",
      "        -1.1583e-01,  2.5421e-02, -6.9419e-03,  1.1042e-01,  2.5280e-02,\n",
      "         6.9696e-02, -1.5054e-02,  1.8588e-01, -1.2759e-01, -1.3206e-01,\n",
      "        -1.6582e-01,  2.2370e-01, -3.8540e-01,  4.5370e-02, -5.7096e-02,\n",
      "         3.9687e-02, -5.4140e-02,  1.6973e-01,  9.8014e-02,  1.0199e-01,\n",
      "         5.7703e-02,  3.5835e-02,  5.1577e-02,  1.9143e-01,  1.8817e-01,\n",
      "         2.0529e-02,  4.8326e-02, -1.2691e-01,  1.1390e-01, -4.9180e-02,\n",
      "         4.3645e-02,  3.5955e-02,  2.4767e-01,  2.0649e-01,  9.3558e-02,\n",
      "        -2.2533e-01, -8.5613e-02, -1.2248e-01, -2.9777e-02,  7.8660e-02,\n",
      "        -1.1236e-02, -1.9922e-01, -3.4905e-03, -3.6305e-02,  1.3839e-01,\n",
      "        -1.5070e-01,  1.0984e-01,  3.0242e-04, -1.2614e-02, -1.0891e-02,\n",
      "         2.7758e-02, -8.7579e-02,  9.2285e-02,  1.8383e-01,  1.9095e-04,\n",
      "         1.9806e-01,  2.6551e-01, -4.2062e-02,  1.0086e-01, -3.0348e-04,\n",
      "        -1.3799e-01,  9.3302e-03,  5.7030e-05, -2.1102e-01,  1.6252e-01,\n",
      "         2.3552e-01, -7.8544e-02,  1.9457e-01, -8.1422e-02, -2.0697e-01,\n",
      "        -1.3599e-01,  1.9197e-01, -5.7078e-02, -1.3957e-01,  1.9618e-01,\n",
      "        -1.7160e-01,  1.8513e-01, -2.4886e-02, -4.8660e-02, -2.7866e-03,\n",
      "        -3.1498e-02, -9.6951e-03, -1.0817e-01,  1.8816e-01, -4.8699e-02,\n",
      "        -1.7686e-01,  6.8372e-02, -7.5191e-03,  1.5600e-01,  6.0893e-02,\n",
      "        -2.5812e-01,  4.9405e-02,  4.6499e-02, -1.1760e-01, -5.1740e-02,\n",
      "         1.5161e-01, -4.3386e-02, -1.4600e-02,  1.8550e-01, -1.2397e-01,\n",
      "         2.2192e-01,  1.4094e-01,  1.0348e-01, -9.5488e-02, -4.0207e-01,\n",
      "        -8.0837e-03, -7.5141e-03,  4.7550e-02, -1.2667e-01,  1.8151e-01,\n",
      "        -1.1514e-01,  1.3927e-02, -6.2727e-02, -1.6647e-01,  1.1580e-01,\n",
      "         3.3476e-02, -7.8001e-02,  3.1218e-01,  1.7393e-01,  1.2961e-01,\n",
      "        -2.0569e-01,  1.6789e-01,  1.5392e-01,  2.2802e-01, -1.0459e-01,\n",
      "         1.3379e-01,  3.4222e-01, -7.0782e-02,  7.2140e-02,  1.8287e-01,\n",
      "         2.5049e-02,  9.3430e-02,  3.6521e-02,  1.4019e-01,  1.1165e-01,\n",
      "        -5.6410e-02, -4.2902e-02,  1.1084e-01,  1.2341e-01, -9.0253e-02,\n",
      "        -1.0624e-01, -1.0747e-01,  1.4199e-01, -2.5435e-01,  6.4154e-02,\n",
      "         2.5627e-02, -2.6009e-02, -5.7520e-02,  8.8595e-02,  1.1214e-02,\n",
      "        -1.6003e-01, -1.6315e-01,  3.2432e-02,  4.3478e-02, -1.5451e-01,\n",
      "        -1.3527e-01,  1.2281e-01, -1.3596e-01,  1.8736e-01,  1.0970e-01,\n",
      "        -4.2530e-02,  2.1186e-01, -3.4840e-02,  2.5089e-02,  9.0492e-02,\n",
      "         2.7376e-02, -1.4922e-01,  1.0662e-01, -1.0367e-01, -2.8011e-01,\n",
      "        -1.1798e-01, -3.3637e-02, -1.0828e-01,  1.5321e-01,  9.9958e-03,\n",
      "        -7.2900e-02,  1.3216e-01,  2.2967e-02,  2.0116e-01,  2.5776e-02,\n",
      "         2.0650e-01,  6.9342e-03, -8.2201e-03,  2.4836e-01,  9.9197e-02,\n",
      "        -1.5933e-02, -6.2824e-02,  5.0447e-02,  1.0979e-01, -1.3801e-01,\n",
      "        -5.5524e-02,  2.7145e-01,  5.5314e-02, -6.8747e-02,  1.2883e-01,\n",
      "         8.8454e-03,  4.4008e-02,  8.4111e-02,  2.2502e-01, -2.3679e-01,\n",
      "         6.0853e-02,  5.4522e-02, -3.2674e-02,  1.4477e-01, -2.7915e-01,\n",
      "        -3.8783e-02,  4.1667e-02,  8.6740e-02,  1.0651e-01,  1.8408e-01,\n",
      "         3.3919e-01, -1.1945e-01, -1.4368e-01,  2.3640e-02,  7.7719e-02,\n",
      "        -1.0014e-01, -7.1230e-02,  1.9574e-01, -1.0707e-01, -1.2946e-01,\n",
      "         4.3760e-02,  2.1360e-01, -1.8363e-01, -4.1565e-02, -1.8846e-02,\n",
      "        -1.4503e-01, -1.1408e-01,  2.0049e-01,  2.9413e-01,  5.2422e-02,\n",
      "         3.5637e-01,  1.9756e-01, -2.1436e-02, -8.7361e-02, -7.5656e-02,\n",
      "        -3.5728e-02,  1.2041e-01, -1.4461e-02,  8.0900e-02,  8.8619e-02,\n",
      "        -8.8588e-02,  1.5192e-01,  2.7496e-02, -1.0980e-01, -4.1808e-02,\n",
      "        -2.1512e-02,  7.5819e-03, -6.6443e-02, -1.8524e-02,  2.3393e-01,\n",
      "         1.7694e-01, -4.6912e-02,  2.1378e-02,  7.7050e-02, -2.3778e-01,\n",
      "        -6.8167e-02,  1.6525e-01, -2.7475e-02,  2.4578e-02, -1.9117e-01,\n",
      "        -1.2740e-01, -8.4107e-02, -1.6877e-02,  1.7691e-03,  4.6460e-01,\n",
      "        -5.7348e-02, -1.0554e-01,  2.1127e-01, -7.3092e-02, -5.3511e-02,\n",
      "        -1.1735e-01, -1.0802e-01, -3.1638e-02, -2.0578e-02, -6.7390e-02,\n",
      "         1.2724e-01,  2.2906e-02, -5.0559e-02, -6.9700e-02, -2.9621e-02,\n",
      "        -7.7158e-02,  1.4772e-02,  6.6124e-02,  1.6396e-01,  3.8393e-03,\n",
      "        -1.9431e-02, -1.5208e-02, -1.4870e-01,  1.7473e-01, -8.5583e-02,\n",
      "         1.7547e-01,  3.8608e-02,  2.2389e-02,  1.2685e-02,  5.7646e-02,\n",
      "         4.5324e-01,  1.1891e-02,  2.9965e-01, -3.5207e-02,  1.3706e-01,\n",
      "        -1.3169e-01,  5.3566e-02, -4.7726e-02, -2.5261e-01, -1.8515e-02,\n",
      "        -2.0234e-02,  2.2785e-01, -1.4517e-02, -9.6714e-02, -2.0476e-01,\n",
      "        -2.8637e-02, -5.8518e-02,  3.6016e-02,  7.6112e-02, -6.7809e-02,\n",
      "         6.9695e-02, -1.3373e-01,  4.1939e-02, -7.2937e-02, -6.0425e-02,\n",
      "         1.9581e-01,  2.8905e-01,  4.3932e-02, -2.4506e-01,  6.7594e-02,\n",
      "        -3.2301e-03, -1.2158e-01,  9.7502e-02,  1.5436e-01, -1.3464e-01,\n",
      "        -1.5772e-01, -1.5352e-01,  2.5136e-01,  4.1822e-03,  8.4459e-02,\n",
      "         7.6944e-02, -6.9655e-02,  8.0202e-02, -3.1955e-02, -7.8647e-02,\n",
      "         2.5145e-02, -5.7851e-02, -1.5955e-01,  3.6215e-01,  8.2082e-02,\n",
      "        -1.6262e-01,  2.0867e-01,  1.5025e-01,  5.7820e-02,  2.4917e-01,\n",
      "         7.2791e-02, -7.9191e-02, -1.8771e-02,  8.7543e-02, -1.4942e-01,\n",
      "        -4.9840e-03, -6.4171e-02, -3.9671e-03,  9.6107e-02, -1.5044e-01,\n",
      "        -4.0660e-02,  7.8659e-02, -1.3380e-01,  2.6918e-02, -5.4366e-03,\n",
      "         2.8354e-01,  3.8164e-02, -1.3850e-01, -4.1194e-02, -2.1837e-01,\n",
      "        -1.1546e-01, -1.8933e-01,  4.0294e-02, -1.5917e-01,  8.9685e-02,\n",
      "         4.6947e-02, -1.5076e-01, -6.5885e-02, -2.8206e-01, -6.3023e-02,\n",
      "        -1.2644e-01, -1.0988e-02, -2.6198e-02, -1.1700e-01, -1.6753e-01,\n",
      "        -2.2510e-01, -6.0460e-02, -9.9629e-02, -1.1667e-01, -9.4955e-02,\n",
      "        -1.3160e-01, -1.5148e-01, -4.3755e-03, -1.1367e-01, -4.7612e-02,\n",
      "        -7.9171e-02, -1.7753e-01, -1.5528e-01, -6.5694e-02, -3.6094e-02,\n",
      "        -1.0873e-01, -3.5677e-02, -8.3985e-02,  9.8189e-02, -1.5878e-01,\n",
      "        -1.4075e-01,  1.1992e-01, -2.4394e-01, -9.3614e-03, -2.1212e-01,\n",
      "        -8.6582e-02,  3.3933e-02, -5.8210e-02, -1.1720e-02,  1.5448e-01,\n",
      "        -8.3001e-02,  2.2375e-02, -1.0332e-02,  4.5355e-02, -2.9894e-02,\n",
      "        -9.5369e-02, -1.4360e-01, -2.6762e-02, -6.6259e-02,  4.7874e-02,\n",
      "         7.2803e-02, -2.1322e-01,  5.2334e-02,  6.6345e-03, -7.7261e-02,\n",
      "        -2.9748e-02, -1.8607e-01,  3.7151e-02,  8.8448e-02,  1.0077e-02,\n",
      "         1.8852e-02, -1.5246e-01, -1.7094e-01, -2.1233e-01, -8.2480e-02,\n",
      "         1.6321e-01, -1.7552e-02, -9.8629e-02, -9.1848e-02,  3.1027e-01])\n",
      "tensor([0.0010, 0.0012, 0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0009,\n",
      "        0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0008, 0.0011, 0.0009,\n",
      "        0.0008, 0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0009, 0.0007, 0.0009,\n",
      "        0.0008, 0.0009, 0.0013, 0.0009, 0.0009, 0.0009, 0.0010, 0.0011, 0.0011,\n",
      "        0.0012, 0.0007, 0.0010, 0.0010, 0.0009, 0.0008, 0.0008, 0.0010, 0.0010,\n",
      "        0.0009, 0.0008, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011, 0.0010, 0.0009,\n",
      "        0.0008, 0.0010, 0.0009, 0.0009, 0.0009, 0.0010, 0.0009, 0.0010, 0.0010,\n",
      "        0.0009, 0.0011, 0.0009, 0.0010, 0.0008, 0.0009, 0.0010, 0.0008, 0.0008,\n",
      "        0.0010, 0.0009, 0.0009, 0.0011, 0.0012, 0.0008, 0.0009, 0.0009, 0.0009,\n",
      "        0.0008, 0.0009, 0.0009, 0.0011, 0.0009, 0.0008, 0.0012, 0.0010, 0.0009,\n",
      "        0.0009, 0.0008, 0.0010, 0.0009, 0.0014, 0.0009, 0.0010, 0.0009, 0.0008,\n",
      "        0.0009, 0.0011, 0.0009, 0.0008, 0.0013, 0.0007, 0.0009, 0.0009, 0.0010,\n",
      "        0.0010, 0.0008, 0.0009, 0.0015, 0.0010, 0.0009, 0.0008, 0.0010, 0.0009,\n",
      "        0.0011, 0.0009, 0.0008, 0.0008, 0.0010, 0.0009, 0.0008, 0.0009, 0.0009,\n",
      "        0.0010, 0.0008, 0.0008, 0.0009, 0.0010, 0.0009, 0.0009, 0.0009, 0.0010,\n",
      "        0.0008, 0.0009, 0.0010, 0.0008, 0.0008, 0.0009, 0.0009, 0.0008, 0.0008,\n",
      "        0.0009, 0.0010, 0.0009, 0.0009, 0.0008, 0.0011, 0.0011, 0.0013, 0.0010,\n",
      "        0.0013, 0.0011, 0.0010, 0.0013, 0.0010, 0.0012, 0.0012, 0.0015, 0.0010,\n",
      "        0.0014, 0.0011, 0.0012, 0.0011, 0.0014, 0.0009, 0.0012, 0.0010, 0.0010,\n",
      "        0.0013, 0.0010, 0.0008, 0.0011, 0.0010, 0.0018, 0.0010, 0.0011, 0.0009,\n",
      "        0.0010, 0.0012, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010,\n",
      "        0.0011, 0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010,\n",
      "        0.0010, 0.0011, 0.0012, 0.0009, 0.0009, 0.0009, 0.0011, 0.0014, 0.0012,\n",
      "        0.0020, 0.0017, 0.0010, 0.0009, 0.0008, 0.0013, 0.0009, 0.0015, 0.0011,\n",
      "        0.0011, 0.0008, 0.0010, 0.0015, 0.0011, 0.0008, 0.0015, 0.0011, 0.0010,\n",
      "        0.0009, 0.0009, 0.0009, 0.0010, 0.0011, 0.0012, 0.0012, 0.0009, 0.0010,\n",
      "        0.0009, 0.0010, 0.0012, 0.0011, 0.0010, 0.0011, 0.0011, 0.0011, 0.0010,\n",
      "        0.0010, 0.0011, 0.0008, 0.0012, 0.0012, 0.0012, 0.0009, 0.0011, 0.0012,\n",
      "        0.0009, 0.0013, 0.0009, 0.0011, 0.0012, 0.0015, 0.0010, 0.0012, 0.0012,\n",
      "        0.0010, 0.0008, 0.0012, 0.0010, 0.0012, 0.0017, 0.0015, 0.0009, 0.0009,\n",
      "        0.0010, 0.0010, 0.0010, 0.0012, 0.0009, 0.0009, 0.0010, 0.0009, 0.0008,\n",
      "        0.0009, 0.0009, 0.0010, 0.0011, 0.0011, 0.0011, 0.0011, 0.0008, 0.0009,\n",
      "        0.0008, 0.0009, 0.0009, 0.0008, 0.0009, 0.0009, 0.0009, 0.0008, 0.0012,\n",
      "        0.0011, 0.0009, 0.0010, 0.0008, 0.0009, 0.0008, 0.0009, 0.0009, 0.0008,\n",
      "        0.0009, 0.0009, 0.0009, 0.0010, 0.0010, 0.0009, 0.0008, 0.0010, 0.0009,\n",
      "        0.0008, 0.0008, 0.0008, 0.0009, 0.0009, 0.0010, 0.0008, 0.0009, 0.0009,\n",
      "        0.0009, 0.0008, 0.0009, 0.0009, 0.0010, 0.0010, 0.0008, 0.0009, 0.0016,\n",
      "        0.0011, 0.0013, 0.0008, 0.0009, 0.0009, 0.0009, 0.0009, 0.0011, 0.0011,\n",
      "        0.0008, 0.0009, 0.0010, 0.0010, 0.0010, 0.0012, 0.0009, 0.0009, 0.0007,\n",
      "        0.0008, 0.0009, 0.0009, 0.0009, 0.0011, 0.0011, 0.0010, 0.0009, 0.0012,\n",
      "        0.0010, 0.0011, 0.0012, 0.0011, 0.0009, 0.0010, 0.0011, 0.0011, 0.0011,\n",
      "        0.0013, 0.0010, 0.0009, 0.0011, 0.0007, 0.0010, 0.0010, 0.0008, 0.0010,\n",
      "        0.0009, 0.0009, 0.0012, 0.0010, 0.0010, 0.0009, 0.0007, 0.0009, 0.0010,\n",
      "        0.0009, 0.0011, 0.0011, 0.0009, 0.0010, 0.0009, 0.0010, 0.0013, 0.0012,\n",
      "        0.0010, 0.0011, 0.0009, 0.0010, 0.0009, 0.0011, 0.0010, 0.0010, 0.0009,\n",
      "        0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0009, 0.0012, 0.0011,\n",
      "        0.0009, 0.0009, 0.0011, 0.0010, 0.0008, 0.0015, 0.0012, 0.0010, 0.0010,\n",
      "        0.0011, 0.0010, 0.0009, 0.0008, 0.0011, 0.0009, 0.0009, 0.0009, 0.0011,\n",
      "        0.0011, 0.0010, 0.0012, 0.0010, 0.0011, 0.0008, 0.0009, 0.0011, 0.0008,\n",
      "        0.0012, 0.0010, 0.0010, 0.0009, 0.0011, 0.0009, 0.0009, 0.0010, 0.0010,\n",
      "        0.0009, 0.0009, 0.0012, 0.0008, 0.0010, 0.0011, 0.0009, 0.0013, 0.0008,\n",
      "        0.0011, 0.0010, 0.0012, 0.0012, 0.0009, 0.0008, 0.0011, 0.0010, 0.0010,\n",
      "        0.0010, 0.0012, 0.0012, 0.0011, 0.0010, 0.0009, 0.0011, 0.0017, 0.0010,\n",
      "        0.0012, 0.0011, 0.0013, 0.0012, 0.0010, 0.0009, 0.0009, 0.0008, 0.0009,\n",
      "        0.0010, 0.0011, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009, 0.0008, 0.0011,\n",
      "        0.0008, 0.0012, 0.0009, 0.0008, 0.0010, 0.0009, 0.0011, 0.0011, 0.0012,\n",
      "        0.0011, 0.0008, 0.0010, 0.0010, 0.0011, 0.0008, 0.0009, 0.0010, 0.0007,\n",
      "        0.0010, 0.0011, 0.0011, 0.0011, 0.0010, 0.0009, 0.0012, 0.0011, 0.0008,\n",
      "        0.0008, 0.0010, 0.0012, 0.0013, 0.0010, 0.0009, 0.0009, 0.0009, 0.0013,\n",
      "        0.0009, 0.0009, 0.0008, 0.0010, 0.0012, 0.0008, 0.0010, 0.0010, 0.0010,\n",
      "        0.0009, 0.0009, 0.0011, 0.0012, 0.0009, 0.0012, 0.0010, 0.0009, 0.0007,\n",
      "        0.0011, 0.0009, 0.0009, 0.0015, 0.0008, 0.0010, 0.0009, 0.0010, 0.0010,\n",
      "        0.0011, 0.0010, 0.0011, 0.0010, 0.0012, 0.0009, 0.0009, 0.0008, 0.0012,\n",
      "        0.0007, 0.0010, 0.0009, 0.0010, 0.0009, 0.0012, 0.0011, 0.0011, 0.0010,\n",
      "        0.0010, 0.0010, 0.0012, 0.0012, 0.0010, 0.0010, 0.0009, 0.0011, 0.0009,\n",
      "        0.0010, 0.0010, 0.0013, 0.0012, 0.0011, 0.0008, 0.0009, 0.0009, 0.0010,\n",
      "        0.0011, 0.0010, 0.0008, 0.0010, 0.0010, 0.0011, 0.0009, 0.0011, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0012, 0.0010, 0.0012, 0.0013,\n",
      "        0.0009, 0.0011, 0.0010, 0.0009, 0.0010, 0.0010, 0.0008, 0.0012, 0.0013,\n",
      "        0.0009, 0.0012, 0.0009, 0.0008, 0.0009, 0.0012, 0.0009, 0.0009, 0.0012,\n",
      "        0.0008, 0.0012, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0012,\n",
      "        0.0009, 0.0008, 0.0011, 0.0010, 0.0012, 0.0011, 0.0008, 0.0010, 0.0010,\n",
      "        0.0009, 0.0009, 0.0012, 0.0009, 0.0010, 0.0012, 0.0009, 0.0012, 0.0011,\n",
      "        0.0011, 0.0009, 0.0007, 0.0010, 0.0010, 0.0010, 0.0009, 0.0012, 0.0009,\n",
      "        0.0010, 0.0009, 0.0008, 0.0011, 0.0010, 0.0009, 0.0014, 0.0012, 0.0011,\n",
      "        0.0008, 0.0012, 0.0012, 0.0012, 0.0009, 0.0011, 0.0014, 0.0009, 0.0011,\n",
      "        0.0012, 0.0010, 0.0011, 0.0010, 0.0011, 0.0011, 0.0009, 0.0009, 0.0011,\n",
      "        0.0011, 0.0009, 0.0009, 0.0009, 0.0011, 0.0008, 0.0011, 0.0010, 0.0010,\n",
      "        0.0009, 0.0011, 0.0010, 0.0008, 0.0008, 0.0010, 0.0010, 0.0008, 0.0009,\n",
      "        0.0011, 0.0009, 0.0012, 0.0011, 0.0009, 0.0012, 0.0010, 0.0010, 0.0011,\n",
      "        0.0010, 0.0009, 0.0011, 0.0009, 0.0007, 0.0009, 0.0010, 0.0009, 0.0012,\n",
      "        0.0010, 0.0009, 0.0011, 0.0010, 0.0012, 0.0010, 0.0012, 0.0010, 0.0010,\n",
      "        0.0013, 0.0011, 0.0010, 0.0009, 0.0010, 0.0011, 0.0009, 0.0009, 0.0013,\n",
      "        0.0010, 0.0009, 0.0011, 0.0010, 0.0010, 0.0011, 0.0012, 0.0008, 0.0011,\n",
      "        0.0010, 0.0010, 0.0011, 0.0007, 0.0010, 0.0010, 0.0011, 0.0011, 0.0012,\n",
      "        0.0014, 0.0009, 0.0009, 0.0010, 0.0011, 0.0009, 0.0009, 0.0012, 0.0009,\n",
      "        0.0009, 0.0010, 0.0012, 0.0008, 0.0009, 0.0010, 0.0009, 0.0009, 0.0012,\n",
      "        0.0013, 0.0010, 0.0014, 0.0012, 0.0010, 0.0009, 0.0009, 0.0010, 0.0011,\n",
      "        0.0010, 0.0011, 0.0011, 0.0009, 0.0012, 0.0010, 0.0009, 0.0009, 0.0010,\n",
      "        0.0010, 0.0009, 0.0010, 0.0012, 0.0012, 0.0009, 0.0010, 0.0011, 0.0008,\n",
      "        0.0009, 0.0012, 0.0010, 0.0010, 0.0008, 0.0009, 0.0009, 0.0010, 0.0010,\n",
      "        0.0016, 0.0009, 0.0009, 0.0012, 0.0009, 0.0009, 0.0009, 0.0009, 0.0010,\n",
      "        0.0010, 0.0009, 0.0011, 0.0010, 0.0009, 0.0009, 0.0010, 0.0009, 0.0010,\n",
      "        0.0011, 0.0012, 0.0010, 0.0010, 0.0010, 0.0009, 0.0012, 0.0009, 0.0012,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0016, 0.0010, 0.0013, 0.0010, 0.0011,\n",
      "        0.0009, 0.0010, 0.0009, 0.0008, 0.0010, 0.0010, 0.0012, 0.0010, 0.0009,\n",
      "        0.0008, 0.0010, 0.0009, 0.0010, 0.0011, 0.0009, 0.0011, 0.0009, 0.0010,\n",
      "        0.0009, 0.0009, 0.0012, 0.0013, 0.0010, 0.0008, 0.0011, 0.0010, 0.0009,\n",
      "        0.0011, 0.0012, 0.0009, 0.0008, 0.0008, 0.0013, 0.0010, 0.0011, 0.0011,\n",
      "        0.0009, 0.0011, 0.0010, 0.0009, 0.0010, 0.0009, 0.0008, 0.0014, 0.0011,\n",
      "        0.0008, 0.0012, 0.0011, 0.0010, 0.0013, 0.0011, 0.0009, 0.0010, 0.0011,\n",
      "        0.0009, 0.0010, 0.0009, 0.0010, 0.0011, 0.0009, 0.0009, 0.0011, 0.0009,\n",
      "        0.0010, 0.0010, 0.0013, 0.0010, 0.0009, 0.0009, 0.0008, 0.0009, 0.0008,\n",
      "        0.0010, 0.0008, 0.0011, 0.0010, 0.0009, 0.0009, 0.0007, 0.0009, 0.0009,\n",
      "        0.0010, 0.0010, 0.0009, 0.0008, 0.0008, 0.0009, 0.0009, 0.0009, 0.0009,\n",
      "        0.0009, 0.0009, 0.0010, 0.0009, 0.0009, 0.0009, 0.0008, 0.0008, 0.0009,\n",
      "        0.0010, 0.0009, 0.0010, 0.0009, 0.0011, 0.0008, 0.0009, 0.0011, 0.0008,\n",
      "        0.0010, 0.0008, 0.0009, 0.0010, 0.0009, 0.0010, 0.0012, 0.0009, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0009, 0.0009, 0.0010, 0.0009, 0.0010, 0.0011,\n",
      "        0.0008, 0.0010, 0.0010, 0.0009, 0.0010, 0.0008, 0.0010, 0.0011, 0.0010,\n",
      "        0.0010, 0.0008, 0.0008, 0.0008, 0.0009, 0.0012, 0.0010, 0.0009, 0.0009,\n",
      "        0.0013])\n"
     ]
    }
   ],
   "source": [
    "# move the input and model to GPU for speed if available\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "#using ResNet with default weights\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e61a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "#wget will work on Linux, for windows download this file from link\n",
    "# and save it to your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a07b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "golden retriever 0.0019633634947240353\n",
      "Saluki 0.0018337576184421778\n",
      "Labrador retriever 0.0016934983432292938\n",
      "car mirror 0.0016891126288101077\n",
      "miniature poodle 0.0016858967719599605\n"
     ]
    }
   ],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fd125ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the input and model to GPU for speed if available\n",
    "model1.eval()\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model1.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model2(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "#print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "#print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4337b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tennis ball 0.006101320497691631\n",
      "bucket 0.0056116292253136635\n",
      "hook 0.004805895034223795\n",
      "plunger 0.004577689338475466\n",
      "paper towel 0.0038331716787070036\n"
     ]
    }
   ],
   "source": [
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
